{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 27 марта 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 27 марта, -4 балла после 06:00 3 апреля, -6 баллов после 06:00 10 апреля\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (2 баллов)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы на fit сравнимой со sklearn wine и Speed Dating Data. \n",
    "Для этого используем numpy. \n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Продемонстрируйте умение работать с Pipeline на данных Speed Dating Data и DecisionTreeClassifier. Нужно в pipeline произвести все необходимые преобразования данных и в конце обучить модель. Задание реализуйте под пунктом Задание 3 (уже написано ниже)\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 5 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.feature_importances_ = [0,0,0,0,0]\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "    \n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        return 1 - (np.sum(l_c*l_c, axis=1, keepdims=True)/l_s + np.sum(r_c*r_c, axis=1, keepdims=True)/r_s)/(r_s+l_s)\n",
    "    \n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        return -(np.nansum(l_c*np.log2(l_c/l_s),axis=1, keepdims=True)+np.nansum(r_c*np.log2(r_c/r_s),axis=1, keepdims=True))/(r_s+l_s)\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        return 1 - (np.max(l_c, axis=1, keepdims=True) + np.max(r_c, axis=1, keepdims=True)) / (l_s + r_s)\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(sqrt(n_feature))]\n",
    "        \n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.log2(n_feature))]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return range(n_feature)\n",
    "    \n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # сортирует сортирует связуку x-y по значениям исков\n",
    "        # и находит количество уникальных классов в y\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        class_number = np.unique(y).shape[0]\n",
    "        \n",
    "        # находим индексы в списке sorted_y внутри окна [self.min_samples_split:-self.min_samples_split]\n",
    "        # где происходит изменение числа. например при sorted_y = [1,1,1,3,3,4] в self.min_samples_split = 2\n",
    "        # r_border_ids будет равно [3];\n",
    "        # если в окне только одинаковые классы, то считается, что разбивать по набору признаков х не имеет смысла\n",
    "        # и возвращается gs = infinity\n",
    "        splitted_sorted_y = sorted_y[self.min_samples_split:-self.min_samples_split]\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] != splitted_sorted_y[1:])[0] + (self.min_samples_split + 1)\n",
    "        \n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "        \n",
    "        #eq_el_count -- номера элементов на котором происходит изменение числа\n",
    "        #class_increments -- матрица, строки которой есть количество элементов определённого класса, индекс строки соотве\n",
    "        #тствует классу, индекс столбца -- порядку изменения числа.\n",
    "        #Другими словами номера строк матрицы class_increments и порядковые номера r_border_ids или eq_el_count связаны\n",
    "        #пример: [ 19  22  23  25] - > [  0.  19.   0.] [  3.   0.   0.] [  0.   0.   1.] [  2.   0.   0.]\n",
    "        eq_el_count = r_border_ids - np.append([self.min_samples_split], r_border_ids[:-1])\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_number))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]), sorted_y[r_border_ids - 1]] = 1\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        class_increments[0] = class_increments[0] + np.bincount(sorted_y[:self.min_samples_split], minlength=class_number)\n",
    "\n",
    "        # Этот блок нужен чтобы подготовить все варианты разбиения\n",
    "        # находится кумулятивная сумма class_incremets что есть так же \n",
    "        # для каждого порядкового номера разбиения i в  l_class_count[i] количество классов в левом узле дерева\n",
    "        # r_class_count -- соответственно в правом узле (находится простым вычитанием от изначального распределения классов).\n",
    "        # в таком случае r_border_ids[i] есть количеттво классов в l_class_count[i]. Далее делается решейп r_borderd_id\n",
    "        # для дальнейшего удобного броадкаста. \n",
    "        l_class_count = np.cumsum(class_increments, axis=0)        \n",
    "        r_class_count = np.bincount(sorted_y) - l_class_count\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # считаются gs функции, которая есть абсолютное значение неопределенности, для \n",
    "        # каждого разбиения. Находится разбиене i, при котором достигается минимум неопределённости\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # так как в левом узле должен оказатся признак sorted_x[idx], то нужно поставить границу\n",
    "        # посередине между sorted_x[idx] и sorted_x[idx+1]. возвращается значение неопределённости и эта граница.\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        return gs[idx], (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        bincounts = np.bincount(y, minlength=self.num_class)\n",
    "        probs = bincounts / np.sum(bincounts).astype(float)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        if(self.max_depth is not None and depth == self.max_depth) or \\\n",
    "                self.min_samples_split is not None and x.shape[0] <= self.min_samples_split or\\\n",
    "                np.any(np.argmax(counts) / y.size >= self.sufficient_share):\n",
    "            self.tree[node_id] = (self.LEAF_TYPE, np.argmax(bincounts), probs)\n",
    "            return\n",
    "        \n",
    "        feature_count = x.shape[1]\n",
    "        gs_functions = np.zeros(feature_count) # gs fucntions for each feature, ex.: [1,3,1,4,12,8]\n",
    "        thresholds = np.zeros(feature_count)\n",
    "        \n",
    "        min_class = np.min(y)\n",
    "        y -= min_class\n",
    "        \n",
    "        for i in self.get_feature_ids(feature_count):\n",
    "            gs_functions[i], thresholds[i] = self.__find_threshold(x[:, i], y)\n",
    "            \n",
    "        y += min_class\n",
    "        feature_id = np.argmin(gs_functions)\n",
    "        \n",
    "        if np.isnan(thresholds[feature_id]):\n",
    "            self.tree[node_id] = (self.LEAF_TYPE, np.argmax(bincounts), probs)\n",
    "            return\n",
    "        \n",
    "        x_l, x_r, y_l, y_r = self.__div_samples(x, y, feature_id, thresholds[feature_id])\n",
    "        if y_l.shape[0] == 0 or y_r.shape[0] == 0:\n",
    "            self.tree[node_id] = (self.LEAF_TYPE, np.argmax(bincounts), probs)\n",
    "            return\n",
    "  \n",
    "        self.tree[node_id] = (self.NON_LEAF_TYPE, feature_id, thresholds[feature_id])\n",
    "        \n",
    "        if self.G_function == self.__gini:\n",
    "            I_current = 1 - np.sum((counts / y.size) ** 2)\n",
    "        elif self.G_function == self.__misclass:\n",
    "            I_current = (1 - np.max(counts / y.size))\n",
    "        elif self.G_function == self.__entropy:\n",
    "            I_current = -np.nansum((counts / y.size) * np.log2(counts / y.size))\n",
    "            \n",
    "        self.feature_importances_[feature_id] += (I_current - gs_functions[feature_id])*x.shape[0]\n",
    "\n",
    "        \n",
    "        self.__fit_node(x_l, y_l, 2*node_id + 1, depth + 1)\n",
    "        self.__fit_node(x_r, y_r, 2*node_id + 2, depth + 1)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.feature_importances_ = np.zeros(x.shape[1])\n",
    "        self.__fit_node(x, y, 0, 0) \n",
    "        self.feature_importances_ /= sum([(1 - v[0]) for v in self.tree.itervalues()])\n",
    "        self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89682539682539686"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88571428571428579"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "df = df.iloc[:, :97]\n",
    "df = df.drop(['id'], axis=1)\n",
    "df = df.drop(['idg'], axis=1)\n",
    "df.drop_duplicates(subset=['iid']).gender.value_counts()\n",
    "df = df.drop(['condtn'], axis=1)\n",
    "df = df.drop(['round'], axis=1)\n",
    "df = df.drop(['position', 'positin1'], axis=1)\n",
    "df = df.drop(['order'], axis=1)\n",
    "df = df.drop(['partner'], axis=1)\n",
    "df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "              'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "              'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "             axis=1)\n",
    "df = df.dropna(subset=['age'])\n",
    "df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "df = df.drop(['field'], axis=1)\n",
    "pd.get_dummies(df, columns=['field_cd'],\n",
    "               prefix='field_cd', prefix_sep='=')\n",
    "df = df.drop(['undergra'], axis=1)\n",
    "df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "df = df.drop(['from', 'zipcode'], axis=1)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "df = df.dropna(subset=['date'])\n",
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "df = df.drop(['career'], axis=1)\n",
    "df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "              'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga'], axis=1)\n",
    "df = df.drop(['expnum'], axis=1)\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "    (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T / df.loc[:,\n",
    "                                                                                      'temp_totalsum'].T).T * 100\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "    (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T / df.loc[:,\n",
    "                                                                                      'temp_totalsum'].T).T * 100\n",
    "df = df.drop(['temp_totalsum'], axis=1)\n",
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "    df = df.drop(feat, axis=1)\n",
    "df = df.drop(['wave'], axis=1)\n",
    "df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid']) \\\n",
    "    .drop(['gender'], axis=1) \\\n",
    "    .dropna()\n",
    "df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "    .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "    .dropna()\n",
    "\n",
    "df_female.columns = df_female.columns + '_f'\n",
    "df_female = df_female.drop(['pid_f'], axis=1)\n",
    "df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                       on='pid',\n",
    "                       how='inner')\n",
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 70 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57057712953022621"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')\n",
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57038684027058095"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')\n",
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа %time clf.fit(X_train, y_train)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 999 ms\n"
     ]
    }
   ],
   "source": [
    "# тут должен быть код типа %time my_clf.fit(X_train, y_train)\n",
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def prepare():\n",
    "    global df, X, y\n",
    "    df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv',\n",
    "                     encoding='cp1251')\n",
    "    df = df.iloc[:, :97]\n",
    "    df = df.drop(['id'], axis=1)\n",
    "    df = df.drop(['idg'], axis=1)\n",
    "    df.drop_duplicates(subset=['iid']).gender.value_counts()\n",
    "    df = df.drop(['condtn'], axis=1)\n",
    "    df = df.drop(['round'], axis=1)\n",
    "    df = df.drop(['position', 'positin1'], axis=1)\n",
    "    df = df.drop(['order'], axis=1)\n",
    "    df = df.drop(['partner'], axis=1)\n",
    "    df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "                  'pf_o_sin', 'pf_o_int',\n",
    "                  'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "                  'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "                  'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "                 axis=1)\n",
    "    df = df.dropna(subset=['age'])\n",
    "    df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "    df = df.drop(['field'], axis=1)\n",
    "    pd.get_dummies(df, columns=['field_cd'],\n",
    "                   prefix='field_cd', prefix_sep='=')\n",
    "    df = df.drop(['undergra'], axis=1)\n",
    "    df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "    df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "    df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "    df = df.drop(['from', 'zipcode'], axis=1)\n",
    "    df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "    df = df.drop(['career'], axis=1)\n",
    "    df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "                  'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga'], axis=1)\n",
    "    df = df.drop(['expnum'], axis=1)\n",
    "    df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "    df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "        (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T / df.loc[:,\n",
    "                                                                                          'temp_totalsum'].T).T * 100\n",
    "    df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "    df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "        (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T / df.loc[:,\n",
    "                                                                                          'temp_totalsum'].T).T * 100\n",
    "    df = df.drop(['temp_totalsum'], axis=1)\n",
    "    for i in [4, 5]:\n",
    "        feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "                'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "                'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "        if i != 4:\n",
    "            feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "        df = df.drop(feat, axis=1)\n",
    "    df = df.drop(['wave'], axis=1)\n",
    "    df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid']) \\\n",
    "        .drop(['gender'], axis=1) \\\n",
    "        .dropna()\n",
    "    df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "        .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "        .dropna()\n",
    "\n",
    "    df_female.columns = df_female.columns + '_f'\n",
    "    df_female = df_female.drop(['pid_f'], axis=1)\n",
    "    df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                           on='pid',\n",
    "                           how='inner')\n",
    "    df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "    X = df_pair.iloc[:, 1:].values\n",
    "    y = df_pair.iloc[:, 0].values\n",
    "\n",
    "def split():\n",
    "    global X_train, y_train, X_test, y_test, X, y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5533955076842243"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "X\n",
    "y\n",
    "X_train\n",
    "X_test\n",
    "y_train\n",
    "y_test\n",
    "\n",
    "pipeline = Pipeline([(\"prepare\", prepare()),\n",
    "                     (\"split\", split()),\n",
    "                     (\"classifier\", DecisionTreeClassifier(min_samples_split=2))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "f1_score(y_pred=pipeline.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "int_corr      0.067116\n",
      "field_cd_f    0.034002\n",
      "fun1_1_f      0.032692\n",
      "income        0.032090\n",
      "exphappy      0.030013\n",
      "attr3_1       0.027378\n",
      "income_f      0.024830\n",
      "sinc3_1       0.023442\n",
      "sinc1_1       0.023389\n",
      "amb2_1        0.022685\n",
      "dtype: float64\n",
      "MyDecisionTreeClassifier\n",
      "date          0.119831\n",
      "race          0.099822\n",
      "attr3_1       0.077143\n",
      "shar1_1_f     0.068404\n",
      "fun1_1        0.061171\n",
      "go_out        0.049140\n",
      "int_corr      0.045991\n",
      "age           0.045368\n",
      "amb3_1_f      0.042915\n",
      "imprelig_f    0.041018\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "my_clf = MyDecisionTreeClassifier()\n",
    "my_clf.fit(X_train, y_train)\n",
    "\n",
    "features_names = df_pair.columns[1:]\n",
    "print('DecisionTreeClassifier')\n",
    "print(pd.Series(index=features_names, data=clf.feature_importances_).sort_values(ascending=False).head(10))\n",
    "print('MyDecisionTreeClassifier')\n",
    "print(pd.Series(index=features_names, data=my_clf.feature_importances_).sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'min_samples_split': 8, 'criterion': 'gini'}\n",
      "0.837732703529\n"
     ]
    }
   ],
   "source": [
    "param_dist = {'max_features': ['auto', 'log2', None],\n",
    "              \"min_samples_split\": range(2, 12),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "gscv = GridSearchCV(RandomForestClassifier(n_estimators=10), param_dist)\n",
    "gscv.fit(X_train, y_train)\n",
    "print(gscv.best_params_)\n",
    "print(gscv.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
